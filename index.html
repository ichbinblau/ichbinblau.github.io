<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta property="og:type" content="website">
<meta property="og:title" content="ichbinblau">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="ichbinblau">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ichbinblau">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>ichbinblau</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ichbinblau</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/19/Running-the-NCSDK-examples-in-Docker/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ichbinblau">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ichbinblau">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/19/Running-the-NCSDK-examples-in-Docker/" itemprop="url">Running the NCSDK examples in Docker</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-12-19T14:41:32+08:00">
                2017-12-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>In Movidius’s official documentation, Docker container is not an explicitly supported platform (as of Dec, 2017). I am working for Intel and I decided to test whether the NCS (Neural Compute Stick) can be accessed from a container. Benefiting from this <a href="https://ncsforum.movidius.com/discussion/315/linux-virtual-environment-for-ncs" target="_blank" rel="external">discussion thread</a>, I was able to run the ncsdk examples in a Docker container. Here are the steps I took to make it happen. </p>
<h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><p>My test environment:</p>
<ul>
<li>Host: Ubuntu 16.04.2</li>
<li>Docker: docker-ce v17.09</li>
<li>Movidius compute stick attached</li>
</ul>
<h2 id="Build-Docker-image-with-ncsdk-installed"><a href="#Build-Docker-image-with-ncsdk-installed" class="headerlink" title="Build Docker image with ncsdk installed"></a>Build Docker image with ncsdk installed</h2><p>The <a href="https://github.com/ichbinblau/ncsdk_container/blob/master/Dockerfile" target="_blank" rel="external">Dockerfile</a> I used to install the ncsdk and make the examples is based on instructions from <a href="https://github.com/hughdbrown/movidius/tree/master/docker" target="_blank" rel="external">hughdbrown</a>. I created a repo and uploaded the Dockerfile and its config files to it. You may check this <a href="https://github.com/ichbinblau/ncsdk_container" target="_blank" rel="external">branch</a>, and build the Docker image yourself. Run:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/ichbinblau/ncsdk_container </div><div class="line">cd ncsdk_container</div><div class="line">sudo docker build -t the_image_name .</div></pre></td></tr></table></figure>
<p><strong>Note</strong>: The ncsdk installer currently does not honor any proxy setting options. Build the image without being behind a proxy or the build step may get stuck installing python dependencies from the internet. </p>
<h2 id="Run-ncsdk-examples-in-Docker"><a href="#Run-ncsdk-examples-in-Docker" class="headerlink" title="Run ncsdk examples in Docker"></a>Run ncsdk examples in Docker</h2><p>The ncforum provided a way to run a Docker container which is accessible to the NCS. Host network mode is recommended to make the USB compute stick visible. </p>
<blockquote>
<p>Some users have had USB related problems using the Intel NCS within a Docker environment. We have found that including the <code>--net=host</code> flag can help make the device manager events visible to libusb in a Docker environment. </p>
</blockquote>
<p>Therefore, I used this command:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo docker run --rm --net=host -it -v /etc/apt/apt.conf:/etc/apt/apt.conf:ro --privileged -v /dev:/dev:shared -v /media/data2/NCS/:/media/data2/NCS/ the_image_name:the_image_tag /bin/bash</div></pre></td></tr></table></figure>
<p>This leads to an interactive terminal in the container looking like this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">movidius@theresa-ubuntu:~/ncsdk$</div></pre></td></tr></table></figure>
<p>Inside the container, you can run the examples to test access to the NCS.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cd examples/apps/hello_ncs_cpp/</div><div class="line">make help</div><div class="line">make hello_ncs_cpp</div><div class="line">make run</div></pre></td></tr></table></figure>
<p>Check the result by looking at the command outputs, such as this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">movidius@theresa-ubuntu:~/ncsdk/examples/apps/hello_ncs_cpp$ make run</div><div class="line"></div><div class="line">making hello_ncs_cpp</div><div class="line">g++ cpp/hello_ncs.cpp -o cpp/hello_ncs_cpp -lmvnc</div><div class="line">Created cpp/hello_ncs_cpp executable</div><div class="line"></div><div class="line">making run</div><div class="line">cd cpp; ./hello_ncs_cpp; cd ..</div><div class="line">Hello NCS! Device opened normally.</div><div class="line">Goodbye NCS!  Device Closed normally.</div><div class="line">NCS device working.</div></pre></td></tr></table></figure>
<p>You may continue to test the other model with different frameworks such as caffe and tensorflow under the <code>examples</code> folder. </p>
<h2 id="Known-issue"><a href="#Known-issue" class="headerlink" title="Known issue"></a>Known issue</h2><p>When I exit the container and try to create the container again, I got the following error:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">theresa@theresa-ubuntu:~$ sudo docker run -it --rm --net=host --name=ncsdk -v /etc/apt/apt.conf:/etc/apt/apt.conf:ro --privileged -v /dev:/dev:shared -v /media/data2/NCS/:/media/data2/NCS/ xshan1/ncsdk_container:latest /bin/bash</div><div class="line">docker: Error response from daemon: oci runtime error: container_linux.go:265: starting container process caused &quot;process_linux.go:368: container init caused \&quot;open /dev/console: input/output error\&quot;&quot;.</div></pre></td></tr></table></figure>
<p>It looks like <code>/dev/console</code> is locked and not released when the container is destroyed. The workaround I found was to reboot the system to release the lock.  </p>
<h2 id="Comments-welcome"><a href="#Comments-welcome" class="headerlink" title="Comments welcome"></a>Comments welcome</h2><p>This is quick example of using Docker containers to access the NCS.  I welcome your comments and corrections.</p>
<hr>
<p>References:</p>
<ol>
<li><a href="https://ncsforum.movidius.com/discussion/315/linux-virtual-environment-for-ncs" target="_blank" rel="external">https://ncsforum.movidius.com/discussion/315/linux-virtual-environment-for-ncs</a></li>
<li><a href="https://github.com/hughdbrown/movidius/tree/master/docker" target="_blank" rel="external">https://github.com/hughdbrown/movidius/tree/master/docker</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/06/Bring-up-Kubernetes-v1-8-4-in-Ubuntu-16-04-with-Kubeadm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ichbinblau">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ichbinblau">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/06/Bring-up-Kubernetes-v1-8-4-in-Ubuntu-16-04-with-Kubeadm/" itemprop="url">Bring up Kubernetes v1.8.4 on Ubuntu 16.04 LTS with kubeadm</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-12-06T17:20:04+08:00">
                2017-12-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Kubernetes has released v1.8 since Sepetember 2017. The former installation steps for v1.75 are not compatible to the new version. The article is to document the steps I took to install Kubernetes cluster on Ubuntu Server 16.04 LTS with <a href="https://kubernetes.io/docs/admin/kubeadm/" target="_blank" rel="external">kubeadm</a>. The steps are tested by installing Kubernetes v1.8.4.   </p>
<h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><p>Install Ubuntu Server 16.04 LTS using the HWE kernel (version 4.10) option. The HWE kernel is version 4.10 and aims to support newer platforms, while the Ubuntu Sever 16.04 LTS standard kernel is version 4.40. </p>
<h4 id="Environment-Preparation"><a href="#Environment-Preparation" class="headerlink" title="Environment Preparation"></a>Environment Preparation</h4><p>Setup proxy if you work behind the corporate network as Kubeadm uses the system proxy to download components. Put the following settings in the <code>$HOME/.bashrc</code> file.  Be mindful to put the master host’s IP address in the no_proxy list.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">export http_proxy=http://proxy_ip:proxy_port/</div><div class="line">export https_proxy=http://proxy_ip:proxy_port/</div><div class="line">export ftp_proxy=http://proxy_ip:proxy_port/</div><div class="line">export no_proxy=192.168.1.102,192.168.1.103,192.168.1.101,192.168.1.104,127.0.0.1,localhost,loadbalancer,gateway1,gateway2,gateway3</div></pre></td></tr></table></figure></p>
<p>And check your proxy settings:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">env | grep proxy</div></pre></td></tr></table></figure></p>
<p>Next perform a software update for all packages before you continue with the cluster installation.</p>
<h4 id="Install-latest-OS-updates"><a href="#Install-latest-OS-updates" class="headerlink" title="Install latest OS updates"></a>Install latest OS updates</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt update &amp;&amp; sudo apt upgrade</div></pre></td></tr></table></figure>
<h4 id="Disable-Swap"><a href="#Disable-Swap" class="headerlink" title="Disable Swap"></a>Disable Swap</h4><p>Since v1.8, it is required to turn swap off. Otherwise, kubelet service cannot be started. We disable system swap by run this command:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo swapoff -a</div></pre></td></tr></table></figure></p>
<h4 id="System-Configuration"><a href="#System-Configuration" class="headerlink" title="System Configuration"></a>System Configuration</h4><p>We suppose that we have four servers ready, one as k8s master and the other three as k8s nodes.<br>Configure local DNS in <code>/etc/hosts</code>.  Map the IP address with host names.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">192.168.1.102 loadbalancer</div><div class="line">192.168.1.101 gateway1</div><div class="line">192.168.1.103 gateway2</div><div class="line">192.168.1.104 gateway3</div></pre></td></tr></table></figure></p>
<h2 id="Install-Kubernetes-version-1-8-4-on-each-of-your-hosts"><a href="#Install-Kubernetes-version-1-8-4-on-each-of-your-hosts" class="headerlink" title="Install Kubernetes version 1.8.4 on each of your hosts."></a>Install Kubernetes version 1.8.4 on each of your hosts.</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt install -y kubeadm kubelet kubectl</div></pre></td></tr></table></figure>
<h2 id="Install-Docker-version-17-09-on-each-of-your-hosts"><a href="#Install-Docker-version-17-09-on-each-of-your-hosts" class="headerlink" title="Install Docker version 17.09 on each of your hosts"></a>Install Docker version 17.09 on each of your hosts</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update</div><div class="line">sudo apt-get install -y \</div><div class="line">    apt-transport-https \</div><div class="line">    ca-certificates \</div><div class="line">    curl \</div><div class="line">    software-properties-common</div><div class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</div><div class="line">sudo add-apt-repository \</div><div class="line">   &quot;deb [arch=amd64] https://download.docker.com/linux/$(. /etc/os-release; echo &quot;$ID&quot;) \</div><div class="line">   $(lsb_release -cs) \</div><div class="line">   stable&quot;</div><div class="line">sudo apt-get update &amp;&amp; sudo apt-get install -y docker-ce=$(apt-cache madison docker-ce | grep 17.09 | head -1 | awk &apos;&#123;print $3&#125;&apos;)</div><div class="line">sudo systemctl enable docker</div></pre></td></tr></table></figure>
<p>And ensure that the service is up and running:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo systemctl status docker</div></pre></td></tr></table></figure></p>
<p><strong>Note</strong>: Make sure that the cgroup driver used by kubelet is the same as the one used by Docker. To ensure compatibility you can either update Docker settings (like what the <a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#installing-docker" target="_blank" rel="external">official document</a> recommends) or update kubelet setting by adding the setting option below to  file <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Environment=&quot;KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs&quot;</div></pre></td></tr></table></figure></p>
<p>After updating the service file (for either kubelet or docker service), do remember to reload the configuration and restart the service<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo systemctl daemon-reload</div><div class="line">sudo systemctl restart docker</div><div class="line">sudo systemctl restart kubelet</div></pre></td></tr></table></figure></p>
<p>Other than that, it is highly recommended that you use the <code>overlay2</code> driver which is faster and stronger than other docker storage drivers. You may follow the instructions <a href="https://docs.docker.com/engine/userguide/storagedriver/overlayfs-driver/#prerequisites" target="_blank" rel="external">here</a> to complete the installation. </p>
<h2 id="Initialize-Kubernetes-Master"><a href="#Initialize-Kubernetes-Master" class="headerlink" title="Initialize Kubernetes Master"></a>Initialize Kubernetes Master</h2><p>On the master node (load balancer), if you run as root, do<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubeadm init --apiserver-advertise-address=192.168.1.102 --pod-network-cidr=10.244.0.0/16 --skip-preflight-checks</div></pre></td></tr></table></figure></p>
<p>If you run as a normal user, do<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo -E kubeadm init --apiserver-advertise-address=192.168.1.102 --pod-network-cidr=10.244.0.0/16 --skip-preflight-checks</div></pre></td></tr></table></figure></p>
<p>If <code>--apiserver-advertise-address</code> is not specified, it auto-detects the network interface to advertise the master. Better to set the argument if there are more than one network interface.<br><code>--pod-network-cidr</code>is to specify the virtual IP range for the third party network plugin.<br>Set <code>--use-kubernetes-version</code>if you want to use a specific Kubernetes version.<br>To start using your cluster, you need to run (as a regular user):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">mkdir -p $HOME/.kube</div><div class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</div><div class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</div></pre></td></tr></table></figure></p>
<p>By default, your cluster will not schedule pods on the master for security reasons. Expand your load capacity if you want to schedule pods on your master.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl taint nodes --all node-role.kubernetes.io/master-</div></pre></td></tr></table></figure></p>
<h2 id="Install-WeaveNet-Pod-Network-Plugin"><a href="#Install-WeaveNet-Pod-Network-Plugin" class="headerlink" title="Install WeaveNet Pod Network Plugin"></a>Install WeaveNet Pod Network Plugin</h2><p>A pod network add-on is supposed to be installed in order that pods can communicate with each other.<br>Set <code>/proc/sys/net/bridge/bridge-nf-call-iptables</code> to 1 by adding <code>net.bridge.bridge-nf-call-iptables=1</code> to file <code>/etc/sysctl.d/k8s.conf</code> in order to pass bridged IPv4 traffic to iptables’ chains.<br>And run the following command to make it take effect:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo sysctl -p /etc/sysctl.d/k8s.conf</div></pre></td></tr></table></figure></p>
<p>Then run:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl apply -f &quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d &apos;\n&apos;)&quot;</div></pre></td></tr></table></figure></p>
<p>Check the status of Weave pods and make sure that it is in <code>Running</code> state:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl get pod --all-namespaces -o wide</div></pre></td></tr></table></figure></p>
<h2 id="Join-Nodes-to-Cluster"><a href="#Join-Nodes-to-Cluster" class="headerlink" title="Join Nodes to Cluster"></a>Join Nodes to Cluster</h2><p>Get the cluster token on master:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo kubeadm token list</div></pre></td></tr></table></figure></p>
<p>Since Kubernetes v1.8, the token is only valid for 24 hours. You may generate another token if the previous one gets expired.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo kubeadm token generate</div></pre></td></tr></table></figure></p>
<p>Run the commands below on each of the nodes:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo kubeadm join --token 15491e.0c0c9c99dfbbe690 192.168.1.102:6443 --discovery-token-ca-cert-hash sha256:82a08ef9c830f240e588a26a8ff0a311e6fe3127c1ee4c5fc019f1369007c0b7 --skip-preflight-checks</div></pre></td></tr></table></figure></p>
<p>Replace the token <code>e5e6d6.6710059ca7130394</code> and the sha256 hash with the actual token and hash got from <code>kubeadm init</code> command.<br>“Pub key validation” can be skipped passing <code>--discovery-token-unsafe-skip-ca-verification flag</code> instead of using <code>--discovery-token-ca-cert-hash</code> but the security would be weakened;<br>And check whether nodes joins the cluster successfully.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl get nodes</div></pre></td></tr></table></figure></p>
<h2 id="Install-Dashboard-Add-on"><a href="#Install-Dashboard-Add-on" class="headerlink" title="Install Dashboard Add-on"></a>Install Dashboard Add-on</h2><p>Create the dashboard pod :<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</div></pre></td></tr></table></figure></p>
<p>To start using Dashboard run following command:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kubectl proxy --address=&quot;&lt;ip-addr-listen-on&gt;&quot; -p &lt;listening-port&gt;</div><div class="line">eg. $kubectl proxy –-address=&quot;192.168.1.102&quot; –p 8001</div></pre></td></tr></table></figure></p>
<p>Then access the dashboard at <a href="http://192.168.1.102:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/" target="_blank" rel="external">http://192.168.1.102:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a>. Replace the IP address with the actually IP you are using.<br>There are a couple of ways to login <a href="https://github.com/kubernetes/dashboard/wiki/Access-control#authentication" target="_blank" rel="external">here</a>. For development purpose, you may simply grant full admin privileges to Dashboard’s Service Account by creating below ClusterRoleBinding. Copy the contents below and save as a file named <code>dashboard-admin.yaml</code>.  Use <code>kubectl create -f dashboard-admin.yaml</code> to deploy it. Afterwards you can use Skip option on login page to access Dashboard.</p>
<h2 id="Tear-Down"><a href="#Tear-Down" class="headerlink" title="Tear Down"></a>Tear Down</h2><p>Firstly, <a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#tear-down" target="_blank" rel="external">drain</a> the nodes on the master or wherever credential is configured. It does a graceful termination and marks the node as unschedulable.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets</div><div class="line">kubectl delete node &lt;node name&gt;</div></pre></td></tr></table></figure></p>
<p>Then on the node to be removed, remove all the configuration files and settings<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo kubeadm reset</div></pre></td></tr></table></figure></p>
<h2 id="Diagnose"><a href="#Diagnose" class="headerlink" title="Diagnose"></a>Diagnose</h2><ul>
<li><p>Check services and pods status. <code>kube-system</code> is the default namespace for system-level pods. You may also pass other specific namespaces. Use <code>--all-namespaces</code> to check all namespaces</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl get po,svc -n kube-system</div></pre></td></tr></table></figure>
<p>This is how the output looks like:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">NAME                                       READY     STATUS    RESTARTS   AGE</div><div class="line">po/etcd-loadbalancer                       1/1       Running   0          9m</div><div class="line">po/kube-apiserver-loadbalancer             1/1       Running   0          9m</div><div class="line">po/kube-controller-manager-loadbalancer    1/1       Running   0          10m</div><div class="line">po/kube-dns-545bc4bfd4-2qvkk               3/3       Running   0          10m</div><div class="line">po/kube-proxy-6rk26                        1/1       Running   0          10m</div><div class="line">po/kube-proxy-qvhmw                        1/1       Running   0          1m</div><div class="line">po/kube-scheduler-loadbalancer             1/1       Running   0          9m</div><div class="line">po/kubernetes-dashboard-7486b894c6-dw8zz   1/1       Running   0          23s</div><div class="line">po/weave-net-s59fw                         2/2       Running   0          3m</div><div class="line">po/weave-net-zsfls                         2/2       Running   1          1m</div><div class="line"> </div><div class="line">NAME                       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE</div><div class="line">svc/kube-dns               ClusterIP   10.96.0.10     &lt;none&gt;        53/UDP,53/TCP   10m</div><div class="line">svc/kubernetes-dashboard   ClusterIP   10.110.76.10   &lt;none&gt;        443/TCP         23s</div></pre></td></tr></table></figure>
</li>
<li><p>Check pods logs. Get pod name from the command above (eg. <code>kubernetes-dashboard-3313488171-tkdtz</code>). Use <code>-c &lt;container_name&gt;</code> if there are more than one containers running in the pod.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl logs &lt;pod_name&gt; -f -n kube-system</div></pre></td></tr></table></figure>
</li>
<li><p>Run commands in the container. Use <code>-c &lt;container_name&gt;</code> if there are more than one containers running in the pod.<br>Run a single command:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl exec &lt;pod_name&gt; -n &lt;namespace&gt; &lt;command_ to_run&gt;</div></pre></td></tr></table></figure>
<p>Enter the container’s shell:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl exec -it &lt;pod_name&gt; -n &lt;namespace&gt; -- /bin/bash</div></pre></td></tr></table></figure>
</li>
<li><p>Check Docker logs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo journalctl -u docker.service -f</div></pre></td></tr></table></figure>
</li>
<li><p>Check kubelet logs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo journalctl -u kubelet.service -f</div></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<p>References:</p>
<ol>
<li><a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/" target="_blank" rel="external">https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/</a></li>
<li><a href="https://kubernetes.io/docs/admin/kubeadm/" target="_blank" rel="external">https://kubernetes.io/docs/admin/kubeadm/</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/21/Run-Applications-in-Kubernetes-Cluster/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ichbinblau">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ichbinblau">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/21/Run-Applications-in-Kubernetes-Cluster/" itemprop="url">Run Applications in Kubernetes Cluster</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-21T17:47:11+08:00">
                2017-09-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><p>We suppose that the Kubernetes cluster is up and running with at least one master and one node.<br>Credential has been properly configured and <code>kubectl</code> can be used on at least one of the hosts.<br>Download all the yaml files from <a href="https://github.com/ichbinblau/SmartHome-Demo.git" target="_blank" rel="external">git repo</a> and switch to the directory that contains configuration files.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ git clone https://github.com/ichbinblau/SmartHome-Demo.git</div><div class="line">$ git checkout k8s</div><div class="line">$ cd SmartHome-Demo/smarthome-web-portal/tools/yamls</div></pre></td></tr></table></figure></p>
<p>In order to accelerate the velocity to download the Docker images, we set up a local Docker registry on master host <code>192.168.1.102</code>.  Here is the command to start a local registry.  Create a local directory to make the docker images persistent.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ sudo mkdir -p /var/lib/registry</div><div class="line">$ sudo docker run -d \</div><div class="line">  -p 5000:5000 \</div><div class="line">  --restart=always \</div><div class="line">  --name registry \</div><div class="line">  -v /var/lib/registry:/var/lib/registry \</div><div class="line">  registry:2</div></pre></td></tr></table></figure></p>
<p>You can check the images in the registry by visiting <code>http://192.168.1.102:5000/v2/_catalog</code></p>
<h2 id="Create-a-New-Namespace"><a href="#Create-a-New-Namespace" class="headerlink" title="Create a New Namespace"></a>Create a New Namespace</h2><p>There are a couple of pre-defined namespaces for different purposes (<code>default</code>, <code>kube-system</code>, <code>kube-public</code>). The <code>namespace.yaml</code> defines a new namespace named <code>iot2cloud</code>. Here is the <code>namespace.yaml</code> file:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">cat namespace.yaml</div><div class="line">apiVersion: v1</div><div class="line">kind: Namespace</div><div class="line">metadata:</div><div class="line">  name: iot2cloud</div></pre></td></tr></table></figure></p>
<p>We create a new namespace for our applications.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl create -f namespace.yaml</div></pre></td></tr></table></figure></p>
<p>  You can check the namespaces<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ kubectl get namespaces</div><div class="line">NAME          STATUS    AGE</div><div class="line">default       Active    1d</div><div class="line">iot2cloud     Active    34s</div><div class="line">kube-public   Active    1d</div><div class="line">kube-system   Active    1d</div></pre></td></tr></table></figure></p>
<p>Voila, we have a new namespace created. In order to simplify the command, we create an alias for the namespace in the context. Alternatively, you can make it persistent by add this to your <code>$HOME/.bashrc</code> file.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ alias kub=&apos;kubectl --namespace iot2cloud&apos;</div></pre></td></tr></table></figure></p>
<h2 id="Attach-Labels-to-Nodes"><a href="#Attach-Labels-to-Nodes" class="headerlink" title="Attach Labels to Nodes"></a>Attach Labels to Nodes</h2><p>In our scenario, some of the hosts are resource-constrained.  We would like to assign all the Cloud applications to the Master and the others to the Nodes.<br><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector" target="_blank" rel="external"><code>nodeSelect</code></a> helps us make this happen. Add labels to categorize the nodes and check the results:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ kubectl label nodes gateway1 platform=minnowboard</div><div class="line">$ kubectl label nodes gateway2 platform=minnowboard</div><div class="line">$ kubectl label nodes gateway3 platform=minnowboard</div><div class="line">$ kubectl label nodes loadbalancer platform=nuc</div><div class="line">$ kubectl get nodes --show-labels</div></pre></td></tr></table></figure></p>
<h2 id="Set-up-MariaDB"><a href="#Set-up-MariaDB" class="headerlink" title="Set up MariaDB"></a>Set up MariaDB</h2><h4 id="Create-secrets-and-configMaps"><a href="#Create-secrets-and-configMaps" class="headerlink" title="Create secrets and configMaps"></a>Create secrets and configMaps</h4><p>Kubernetes introduces <a href="https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-using-kubectl-create-secret" target="_blank" rel="external">secret</a> concept to hold sensitive information such as password, token, key pairs etc.  We keep our MariaDB password in the <code>secret</code> and MariaDB pod will create the password set in the <code>secret</code> for user <code>root</code>.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ MYSQL_ROOT_PASS = your_password_to_use</div><div class="line">$ echo -n $MYSQL_ROOT_PASS &gt; mysql-root-pass.secret</div><div class="line">$ kub create secret generic mysql-root-pass --from-file=mysql-root-pass.secret</div></pre></td></tr></table></figure></p>
<p>You can check that the secret was created like this:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">$ kub get secrets</div><div class="line">NAME                  TYPE                                  DATA      AGE</div><div class="line">mysql-root-pass       Opaque                                1         2m</div><div class="line">$ kub describe secrets/mysql-root-pass</div><div class="line">Name:           mysql-root-pass</div><div class="line">Namespace:      iot2cloud</div><div class="line">Labels:         &lt;none&gt;</div><div class="line">Annotations:    &lt;none&gt;</div><div class="line"></div><div class="line">Type:   Opaque</div><div class="line"></div><div class="line">Data</div><div class="line">====</div><div class="line">mysql-root-pass.secret: 8 bytes</div></pre></td></tr></table></figure></p>
<h4 id="Create-MariaDB-Pod-and-Service"><a href="#Create-MariaDB-Pod-and-Service" class="headerlink" title="Create MariaDB Pod and Service"></a>Create MariaDB Pod and Service</h4><p>Remember to update the <code>image: 192.168.1.102:5000/mariadb:latest</code> in <code>mariadb-master-service.yaml</code> file to the actual repository you use.  <strong>Make the same changes to all the other yaml file.</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ kub create -f mariadb-master-service.yaml # service definition</div><div class="line">$ kub create -f mariadb-master.yaml # deployment definition</div></pre></td></tr></table></figure></p>
<p>Check the pods and service status:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kub get po,svc -o wide</div></pre></td></tr></table></figure></p>
<p>Optionally, if you are going to run MariaDB in master-slave mode.  Ensure that there are more than one nodes labeled <code>nuc</code> in the cluster and run:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ kub create -f mariadb-slave-service.yaml</div><div class="line">$ kub create -f mariadb-slave.yaml</div></pre></td></tr></table></figure></p>
<h2 id="Run-RabbitMQ-Service"><a href="#Run-RabbitMQ-Service" class="headerlink" title="Run RabbitMQ Service"></a>Run RabbitMQ Service</h2><p>RabbitMQ service exposes port 5672 catering for requests.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ kub create -f rabbitmq-service.yaml</div><div class="line">$ kub create -f rabbitmq.yaml</div></pre></td></tr></table></figure></p>
<h2 id="Create-IoT-Rest-API-Service-and-Gateway-Server"><a href="#Create-IoT-Rest-API-Service-and-Gateway-Server" class="headerlink" title="Create IoT Rest API Service and Gateway Server"></a>Create IoT Rest API Service and Gateway Server</h2><p>There are two containers running in one pod, one for iot-rest-api-service and the other is for gateway server.<br>The <code>replica</code> is set to 3, meaning 3 pods would be created evenly on <code>gateway1</code> to <code>gateway3</code> hosts.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ kub create -f smarthome-gateway-service.yaml</div><div class="line">$ kub create -f smarthome-gateway.yaml</div></pre></td></tr></table></figure></p>
<p>You can get the <code>NodePort</code> of Rest API service:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ kub describe svc/gateway | grep NodePort</div><div class="line">Type:                   NodePort</div><div class="line">NodePort:               &lt;unset&gt; 32626/TCP</div></pre></td></tr></table></figure></p>
<p>And browse the REST service via <a href="http://192.168.1.102:32626/api/oic/res" target="_blank" rel="external">http://192.168.1.102:32626/api/oic/res</a></p>
<h2 id="Run-Home-Dashboard"><a href="#Run-Home-Dashboard" class="headerlink" title="Run Home Dashboard"></a>Run Home Dashboard</h2><p>2 pods will be created for home dashboard and database will be initialized after running the commands below:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ kub create -f home-portal-service.yaml</div><div class="line">$ kub create -f home-portal.yaml</div></pre></td></tr></table></figure></p>
<p>Get the NodePort:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ kub describe  svc/home-portal | grep NodePort</div><div class="line">Type:                   NodePort</div><div class="line">NodePort:               home    31328/TCP</div></pre></td></tr></table></figure></p>
<p>Then you are able to login to the home portal via <a href="http://192.168.1.102:31328/" target="_blank" rel="external">http://192.168.1.102:31328/</a> (use the NodePort you got from the <code>kubectl</code> command instead)</p>
<h2 id="Start-Admin-Portal"><a href="#Start-Admin-Portal" class="headerlink" title="Start Admin Portal"></a>Start Admin Portal</h2><p>Run the following commands to start the admin portal. The admin portal can only run on single pod in that the trained models are stored in the local file system and not yet shared between pods.<br>Update the env <code>http_proxy</code> and <code>https_proxy</code> to empty string in <code>admin-portal.yaml</code> if it is not required.  Then run:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ kub create -f admin-portal-service.yaml</div><div class="line">$ kub create -f admin-portal.yaml</div></pre></td></tr></table></figure></p>
<p>Get the NodePort and visit the admin portal.  Next, point the <code>demo</code> gateway’s IP address to the <code>http://gateway.iot2cloud:8000/</code>.</p>
<h2 id="Run-Celery-Worker-and-Trigger-Tasks"><a href="#Run-Celery-Worker-and-Trigger-Tasks" class="headerlink" title="Run Celery Worker and Trigger Tasks"></a>Run Celery Worker and Trigger Tasks</h2><p>Celery worker is simply a worker process thereby no service definition required.  There are two containers in the pod, one for long running tasks and the other for periodic tasks.<br>Run the command below to initialize the worker:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kub create -f celery-worker.yaml</div></pre></td></tr></table></figure></p>
<p>And run this command  to trigger the tasks (I will try to skip this manual step later):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kub exec $(kub get po -l app=celery-worker -o name | cut -d&apos;/&apos; -f2) -c long-task -- python CeleryTask/tasks.py</div></pre></td></tr></table></figure></p>
<h2 id="BKMs"><a href="#BKMs" class="headerlink" title="BKMs"></a>BKMs</h2><p><strong>Restart pods</strong>: in some cases, pods get error or fail to restart. I need to restart the pod to recover the application.  There is no straight command to restart pods. You can restart pods by:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ kub delete pods &lt;pod_to_delete&gt;</div><div class="line">$ kub create -f &lt;yml_file_describing_pod&gt;</div></pre></td></tr></table></figure></p>
<p>If you cannot find the yaml file immediately, run the command below alternatively:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kub get pod &lt;pod_to_delete&gt; -o yaml | kubectl replace --force -f -</div></pre></td></tr></table></figure></p>
<p><strong>Pod log outputs</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kub logs -f  &lt;pod_name&gt; -c &lt;container_name&gt;</div></pre></td></tr></table></figure></p>
<hr>
<p>References:</p>
<ol>
<li><a href="http://agiletesting.blogspot.com/2016/11/running-application-using-kubernetes-on.html" target="_blank" rel="external">http://agiletesting.blogspot.com/2016/11/running-application-using-kubernetes-on.html</a></li>
<li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#overview-of-secrets" target="_blank" rel="external">https://kubernetes.io/docs/concepts/configuration/secret/#overview-of-secrets</a></li>
<li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector" target="_blank" rel="external">https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/20/Setup-Kubernetes-Cluster-on-Centos-7-3-with-Kubeadm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ichbinblau">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ichbinblau">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/20/Setup-Kubernetes-Cluster-on-Centos-7-3-with-Kubeadm/" itemprop="url">Setup Kubernetes Cluster on Centos 7.3 with Kubeadm</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-20T16:29:53+08:00">
                2017-09-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>The article is to document the steps I took to install Kubernetes cluster on Centos 7.3 with <a href="https://kubernetes.io/docs/admin/kubeadm/" target="_blank" rel="external">kubeadm</a>. </p>
<h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><h4 id="System-Configuration"><a href="#System-Configuration" class="headerlink" title="System Configuration"></a>System Configuration</h4><p>We suppose we have four servers ready, one as k8s master and the other three as k8s nodes.<br>Configure local DNS in <code>/etc/hosts</code>.  Map the IP address with host names.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">192.168.1.102 loadbalancer</div><div class="line">192.168.1.101 gateway1</div><div class="line">192.168.1.103 gateway2</div><div class="line">192.168.1.104 gateway3</div></pre></td></tr></table></figure></p>
<h4 id="Environment-Preparation"><a href="#Environment-Preparation" class="headerlink" title="Environment Preparation"></a>Environment Preparation</h4><p>We should have at least two servers with Centos 7.3 pre-installed and keep them in the same subnet.<br>Optionally, setup proxy if you work behind the corporate network as Kubeadm uses the system proxy to download components. Put the following settings in the <code>$HOME/.bashrc</code> file.  Be mindful to put the master host’s IP address in the no_proxy list.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">export http_proxy=http://proxy_ip:proxy_port/</div><div class="line">export https_proxy=http://proxy_ip:proxy_port/</div><div class="line">export ftp_proxy=http://proxy_ip:proxy_port/</div><div class="line">export no_proxy=192.168.1.102,192.168.1.103,192.168.1.101,192.168.1.104,127.0.0.1,localhost,loadbalancer,gateway1,gateway2,gateway3</div></pre></td></tr></table></figure></p>
<p>And check your proxy settings:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ env | grep proxy</div></pre></td></tr></table></figure></p>
<h2 id="Install-kubeadm-and-kubelet-on-each-of-your-hosts"><a href="#Install-kubeadm-and-kubelet-on-each-of-your-hosts" class="headerlink" title="Install kubeadm and kubelet on each of your hosts"></a>Install kubeadm and kubelet on each of your hosts</h2><p>Add k8s repo to the yum source list.  We suppose you run the command as root. For non-root users, please wrap the command with <code>sudo bash -c &#39;&lt;command_to_run&gt;&#39;</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$ cat &lt;&lt; EOF &gt; /etc/yum.repos.d/kubernetes.repo</div><div class="line">[kubernetes]</div><div class="line">name=Kubernetes</div><div class="line">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</div><div class="line">enabled=1</div><div class="line">gpgcheck=1</div><div class="line">repo_gpgcheck=1</div><div class="line">sslverify=0</div><div class="line">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg</div><div class="line">        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</div><div class="line">EOF</div></pre></td></tr></table></figure></p>
<p>Install kubeadm and kubelet on each hosts.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ sudo setenforce 0</div><div class="line">$ sudo yum install -y kubelet kubeadm</div><div class="line">$ sudo systemctl enable kubelet </div><div class="line">$ sudo systemctl start kubelet</div></pre></td></tr></table></figure></p>
<p>By default, the following command installs the latest kubelet and kubeadm. If you want to install a specific version,  you may check the versions.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo yum list kubeadm  --showduplicates |sort -r</div><div class="line">kubeadm.x86_64                        1.7.5-0                        kubernetes</div><div class="line">kubeadm.x86_64                        1.7.4-0                        kubernetes</div></pre></td></tr></table></figure></p>
<p>And install the specific version ( eg. v1.7.5 )<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo yum install kubeadm-1.7.5-0</div></pre></td></tr></table></figure></p>
<h2 id="Install-Docker-on-each-of-your-hosts"><a href="#Install-Docker-on-each-of-your-hosts" class="headerlink" title="Install Docker on each of your hosts"></a>Install Docker on each of your hosts</h2><p>For the time being, Docker version 1.12.x is still the preferred and verified version that Kubernetes officially supported according to it <a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#installing-docker" target="_blank" rel="external">doc</a>.  But this <a href="https://stackoverflow.com/questions/44657320/which-docker-versions-will-k8s-1-7-support" target="_blank" rel="external">thread</a> says that they will add support for Docker 1.13 very soon.<br>We will install Docker 1.12 for now. Use the following command to set up the  repository.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ cat &lt;&lt; EOF &gt; /etc/yum.repos.d/docker.repo</div><div class="line">[dockerrepo]</div><div class="line">name=Docker Repository</div><div class="line">baseurl=https://yum.dockerproject.org/repo/main/centos/7</div><div class="line">enabled=1</div><div class="line">gpgcheck=1</div><div class="line">gpgkey=https://yum.dockerproject.org/gpg</div><div class="line">EOF</div></pre></td></tr></table></figure></p>
<p>Make yum cache and check possible Docker 1.12.x versions<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo yum makecache</div><div class="line">$ sudo yum list docker-engine  --showduplicates |sort -r</div><div class="line">docker-engine.x86_64             1.12.6-1.el7.centos                 dockerrepo</div></pre></td></tr></table></figure></p>
<p>Intall and start Docker service<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo yum install docker-engine-1.12.6 -y </div><div class="line">$ sudo systemctl enable docker </div><div class="line">$ sudo systemctl start docker</div></pre></td></tr></table></figure></p>
<p>Verify if your Docker cgroup driver matches the kubelet config:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo docker info |grep -i cgroup</div><div class="line">$ sudo cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</div></pre></td></tr></table></figure></p>
<p>The default cgroup driver in kubelet config is <code>systemd</code>. If Docker’s cgroup driver is not <code>systemd</code> but <code>cgroupfs</code>.  Update the cgroup driver in <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> to  <code>KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs</code><br>Further, if your Docker runs behind corporate network, set up the proxy in the Docker config:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ cat &lt;&lt; EOF &gt;/etc/systemd/system/docker.service.d/http-proxy.conf</div><div class="line">[Service]</div><div class="line">Environment=&quot;HTTP_PROXY=http://proxy_ip:proxy_port/&quot; &quot;NO_PROXY=localhost,127.0.0.1,192.168.1.102,192.168.1.103,192.168.1.101,192.168.1.104,loadbalancer,gateway1,gateway2,gateway3&quot;</div><div class="line">EOF</div></pre></td></tr></table></figure></p>
<p>Then reload the config:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo systemctl daemon-reload</div><div class="line">$ sudo systemctl restart kubelet</div><div class="line">$ sudo systemctl restart docker  # restart docker if you add proxy config</div></pre></td></tr></table></figure></p>
<h2 id="Initialize-Master"><a href="#Initialize-Master" class="headerlink" title="Initialize Master"></a>Initialize Master</h2><p>On the master node (load balancer), if you run as root, do<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubeadm init --apiserver-advertise-address=192.168.1.102 --pod-network-cidr=10.244.0.0/16</div></pre></td></tr></table></figure></p>
<p>If you run as a normal user, do<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo -E -c &quot;kubeadm init --apiserver-advertise-address=192.168.1.102 --pod-network-cidr=10.244.0.0/16&quot;</div></pre></td></tr></table></figure></p>
<p>If <code>--apiserver-advertise-address</code> is not specified, it auto-detects the network interface to advertise the master. Better to set the argument if there are more than one network interface.<br><code>--pod-network-cidr</code>is to specify the virtual IP range for the third party network plugin. We use <a href="https://github.com/coreos/flannel" target="_blank" rel="external">flannel</a> as our network plugin here.<br>Set <code>--use-kubernetes-version</code>if you want to use specific Kubernetes version.<br>To start using your cluster, you need to run (as a regular user):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">mkdir -p $HOME/.kube</div><div class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</div><div class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</div></pre></td></tr></table></figure></p>
<p>By default, your cluster will not schedule pods on the master for security reasons. Expand your load capacity if you want to schedule pods on your master.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl taint nodes --all node-role.kubernetes.io/master-</div></pre></td></tr></table></figure></p>
<h2 id="Install-Flannel-Pod-Network-Plugin"><a href="#Install-Flannel-Pod-Network-Plugin" class="headerlink" title="Install Flannel Pod Network Plugin"></a>Install Flannel Pod Network Plugin</h2><p>A pod network add-on is supposed to be installed in order that pods can communicate with each other. Run:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kubectl create -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel-rbac.yml</div><div class="line">kubectl apply -f  https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</div></pre></td></tr></table></figure></p>
<p>If there are more than one NIC, refer to the <a href="https://github.com/kubernetes/kubernetes/issues/39701" target="_blank" rel="external">flannel issue 39701</a>.<br>Next, configure Docker with Flannel IP range and settings:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$ sudo mkdir -p /etc/systemd/system/docker.service.d</div><div class="line">$ cat &lt;&lt; EOF &gt; /etc/systemd/system/docker.service.d/flannel.conf</div><div class="line">[Service]</div><div class="line">EnvironmentFile=-/run/flannel/docker</div><div class="line">EOF</div><div class="line">$ sudo cat &lt;&lt; EOF &gt; /run/flannel/docker</div><div class="line">DOCKER_OPT_BIP=&quot;--bip=10.244.0.1/24&quot;</div><div class="line">DOCKER_OPT_IPMASQ=&quot;--ip-masq=false&quot;</div><div class="line">DOCKER_OPT_MTU=&quot;--mtu=1450&quot;</div><div class="line">DOCKER_NETWORK_OPTIONS=&quot; --bip=10.244.0.1/24 --ip-masq=false --mtu=1450&quot;</div><div class="line">EOF</div></pre></td></tr></table></figure></p>
<p>Reload the config:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo systemctl daemon-reload</div><div class="line">$ sudo systemctl restart docker</div></pre></td></tr></table></figure></p>
<p>Check the status of flannel pods and make sure that it is in <code>Running</code> state:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl get pod --all-namespaces -o wide</div></pre></td></tr></table></figure></p>
<h2 id="Join-Nodes-to-Cluster"><a href="#Join-Nodes-to-Cluster" class="headerlink" title="Join Nodes to Cluster"></a>Join Nodes to Cluster</h2><p>Get the cluster token on master:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo kubeadm token list</div></pre></td></tr></table></figure></p>
<p>Run the commands below on each of the nodes:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubeadm join --token e5e6d6.6710059ca7130394 192.168.1.102:6443</div></pre></td></tr></table></figure></p>
<p>Replace <code>e5e6d6.6710059ca7130394</code> with the token got from <code>kubeadm</code> command.<br>And check whether nodes joins the cluster successfully.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl get nodes</div></pre></td></tr></table></figure></p>
<h2 id="Install-Dashboard-Add-on"><a href="#Install-Dashboard-Add-on" class="headerlink" title="Install Dashboard Add-on"></a>Install Dashboard Add-on</h2><p>Create the dashboard pod :<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml</div></pre></td></tr></table></figure></p>
<p>Since Kubernetes v1.6, its API server uses RBAC strategy. The <code>kubernetes-dashboard.yaml</code> does not define an valid ServiceAccount. Create file <code>dashboard-rbac.yaml</code> and bind account <code>system:serviceaccount:kube-system:default</code> with role <code>ClusterRole cluster-admin</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">$ cat &lt;&lt; EOF &gt; dashboard-rbac.yaml </div><div class="line">kind: ClusterRoleBinding</div><div class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</div><div class="line">metadata:</div><div class="line">  name: dashboard-admin</div><div class="line">roleRef:</div><div class="line">  apiGroup: rbac.authorization.k8s.io</div><div class="line">  kind: ClusterRole</div><div class="line">  name: cluster-admin </div><div class="line">subjects:</div><div class="line">- kind: ServiceAccount</div><div class="line">  name: default</div><div class="line">  namespace: kube-system</div><div class="line">EOF</div></pre></td></tr></table></figure></p>
<p>Define the RBAC rules to the pod and check pod state with <code>kubectl get po --all-namespace</code> command after that</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl create -f dashboard-rbac.yaml</div></pre></td></tr></table></figure>
<p>Configure <code>kubernetes-dashboard</code> service to use <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport" target="_blank" rel="external">NodePort</a>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">$ cat &lt;&lt; EOF &gt; dashboard-svc.yaml</div><div class="line">kind: Service</div><div class="line">apiVersion: v1</div><div class="line">metadata:</div><div class="line">  labels:</div><div class="line">    k8s-app: kubernetes-dashboard</div><div class="line">  name: kubernetes-dashboard</div><div class="line">  namespace: kube-system</div><div class="line">spec:</div><div class="line">  type: NodePort</div><div class="line">  ports:</div><div class="line">  - port: 80</div><div class="line">    targetPort: 9090</div><div class="line">  selector:</div><div class="line">    k8s-app: kubernetes-dashboard</div><div class="line">EOF</div><div class="line">$ kubectl apply -f dashboard-svc.yaml</div></pre></td></tr></table></figure></p>
<p>Then get the NodePort.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ kubectl get svc kubernetes-dashboard -n kube-system</div><div class="line">NAME                   CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</div><div class="line">kubernetes-dashboard   10.102.129.68   &lt;nodes&gt;       80:32202/TCP   12h</div></pre></td></tr></table></figure></p>
<p><code>32202</code> in the output is the NodePort. You can visit the dashboard by <code>http://&lt;master-ip&gt;:&lt;node_port&gt;</code> now. In our case, the url is <code>http://192.168.1.102:32202</code>. </p>
<h2 id="Tear-Down"><a href="#Tear-Down" class="headerlink" title="Tear Down"></a>Tear Down</h2><p>Firstly, <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.7/#drain" target="_blank" rel="external">drain</a> the nodes on the master or wherever credential is configured. It does a graceful termination and marks the node as unschedulable.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets</div><div class="line">$ kubectl delete node &lt;node name&gt;</div></pre></td></tr></table></figure></p>
<p>Then on the node to be removed, remove all the configuration files and settings<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo kubeadm reset</div></pre></td></tr></table></figure></p>
<h2 id="Diagnose"><a href="#Diagnose" class="headerlink" title="Diagnose"></a>Diagnose</h2><ul>
<li><p>Check services and pods status. <code>kube-system</code> is the default namespace for system-level pods. You may also pass other specific namespaces. Use <code>--all-namespaces</code> to check all namespaces</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl get po,svc -n kube-system</div></pre></td></tr></table></figure>
<p>This is how the output looks like:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">NAME                                       READY     STATUS    RESTARTS   AGE</div><div class="line">po/etcd-loadbalancer                       1/1       Running   0          1d</div><div class="line">po/kube-apiserver-loadbalancer             1/1       Running   0          1d</div><div class="line">po/kube-controller-manager-loadbalancer    1/1       Running   0          1d</div><div class="line">po/kube-dns-2425271678-zj91n               3/3       Running   0          1d</div><div class="line">po/kube-flannel-ds-w9dvz                   2/2       Running   0          1d</div><div class="line">po/kube-flannel-ds-zn6c4                   2/2       Running   1          1d</div><div class="line">po/kube-proxy-m6nvj                        1/1       Running   0          1d</div><div class="line">po/kube-proxy-w92kx                        1/1       Running   0          1d</div><div class="line">po/kube-scheduler-loadbalancer             1/1       Running   0          1d</div><div class="line">po/kubernetes-dashboard-3313488171-tkdtz   1/1       Running   0          1d</div><div class="line"></div><div class="line">NAME                       CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE</div><div class="line">svc/kube-dns               10.96.0.10      &lt;none&gt;        53/UDP,53/TCP   1d</div><div class="line">svc/kubernetes-dashboard   10.102.129.68   &lt;nodes&gt;       80:32202/TCP    1d</div></pre></td></tr></table></figure>
</li>
<li><p>Check pods logs. Get pod name from the command above (eg. <code>kubernetes-dashboard-3313488171-tkdtz</code>). Use <code>-c &lt;container_name&gt;</code> if there are more than one containers running in the pod. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl logs &lt;pod_name&gt; -f -n kube-system</div></pre></td></tr></table></figure>
</li>
<li><p>Run commands in the container. Use <code>-c &lt;container_name&gt;</code> if there are more than one containers running in the pod.<br>Run a single command:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl exec &lt;pod_name&gt; -n &lt;namespace&gt; &lt;command_ to_run&gt;</div></pre></td></tr></table></figure>
<p>Enter the container’s shell:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl exec -it &lt;pod_name&gt; -n &lt;namespace&gt; -- /bin/bash</div></pre></td></tr></table></figure>
</li>
<li><p>Check Docker logs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo journalctl -u docker.service -f</div></pre></td></tr></table></figure>
</li>
<li><p>Check kubelet logs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo journalctl -u kubelet.service -f</div></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<p>References:</p>
<ol>
<li><a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/" target="_blank" rel="external">https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/</a></li>
<li><a href="https://kubernetes.io/docs/admin/kubeadm/" target="_blank" rel="external">https://kubernetes.io/docs/admin/kubeadm/</a></li>
<li><a href="https://github.com/coreos/flannel" target="_blank" rel="external">https://github.com/coreos/flannel</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="ichbinblau" />
          <p class="site-author-name" itemprop="name">ichbinblau</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">tags</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ichbinblau</span>
</div>
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
Total <span id="busuanzi_value_site_pv"></span> views.
You are the <span id="busuanzi_value_site_uv"></span>th visitor.
<span id="busuanzi_value_page_pv"></span> Hits

        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  

  

  

</body>
</html>
